{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
      "metadata": {
        "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb"
      },
      "source": [
        "# Lab | Tools prompting\n",
        "\n",
        "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b94240",
      "metadata": {
        "id": "14b94240"
      },
      "source": [
        "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
        "\n",
        ":::{.callout-caution}\n",
        "\n",
        "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](/docs/how_to/tool_calling) guide for more information.\n",
        "\n",
        "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
        "\n",
        "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
        "\n",
        "<br>\n",
        "\n",
        "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
      "metadata": {
        "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We'll need to install the following packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
      "metadata": {
        "collapsed": true,
        "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
        "outputId": "a0fd13d7-0a4b-4b90-c83a-ad1ae50de337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.5/127.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ollama openai"
      ],
      "metadata": {
        "id": "oFoUEPKyBhXp",
        "outputId": "1f5dc127-a59a-403d-958f-ac707c2de499",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oFoUEPKyBhXp",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ollama\n",
            "  Downloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/328.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/328.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx<0.28.0,>=0.27.0 (from ollama)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.0)\n",
            "Installing collected packages: h11, httpcore, httpx, openai, ollama\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 ollama-0.2.1 openai-1.35.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E6fE6sXK9dvj",
        "outputId": "6caf5d6d-1a76-41ba-8474-3541d1aaa988",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "E6fE6sXK9dvj",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_H8LZnJT9l8f",
        "outputId": "fe6519a9-a417-4d01-9490-4a2060a09b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_H8LZnJT9l8f",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m783.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn"
      ],
      "metadata": {
        "id": "7sp1Aagt-itd",
        "outputId": "f90db428-b293-4e67-fa98-6dbde8c3c92e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7sp1Aagt-itd",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.3.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->flash-attn)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->flash-attn)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->flash-attn)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->flash-attn)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->flash-attn)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->flash-attn)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->flash-attn)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->flash-attn)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn) (1.3.0)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120889689 sha256=5022ba11d48bf74926da9c16260f4ea2b9bb7f4e29bdb4bd6e1383ad1c55d16f\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n",
            "Successfully built flash-attn\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn\n",
            "Successfully installed flash-attn-2.5.9.post1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
      "metadata": {
        "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3"
      },
      "source": [
        "If you'd like to use LangSmith, uncomment the below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
      "metadata": {
        "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
        "outputId": "07612fb4-de86-4141-8a95-3cb3054e4cfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
      "metadata": {
        "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3"
      },
      "source": [
        "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](/docs/integrations/chat/), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](/docs/how_to/tool_calling) guide.\n",
        "\n",
        "```{=mdx}\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
        "```\n",
        "\n",
        "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](/docs/integrations/chat/ollama/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n"
      ],
      "metadata": {
        "id": "ONNuDJJaBJ8m",
        "outputId": "4ab1415d-0441-42ba-ca32-e88412e44db3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ONNuDJJaBJ8m",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/myscript.sh\n",
        "!echo '#!/bin/bash' > /content/myscript.sh\n",
        "!echo 'ollama serve' >> /content/myscript.sh\n",
        "# Make the script executable\n",
        "!chmod +x /content/myscript.sh\n",
        "!nohup /content/myscript.sh &"
      ],
      "metadata": {
        "id": "3lImZF80BNGo",
        "outputId": "b5eef247-efc5-49ed-85cc-6512106dbb8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3lImZF80BNGo",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi3:medium\n"
      ],
      "metadata": {
        "id": "tNj_dfJdBUgi",
        "outputId": "069387b2-6bcc-4922-b587-c54bed483898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tNj_dfJdBUgi",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 7a3cc4804c64... 100% ▕▏ 7.9 GB                         \n",
            "pulling fa8235e5b48f... 100% ▕▏ 1.1 KB                         \n",
            "pulling 542b217f179c... 100% ▕▏  148 B                         \n",
            "pulling 8dde1baf1db0... 100% ▕▏   78 B                         \n",
            "pulling c266f81b82cd... 100% ▕▏  483 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "removing any unused layers \n",
            "success \u001b[?25h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# Setting up the model, enabling streaming responses, and defining the input messages\n",
        "ollama_response = ollama.chat(model='phi3:medium', messages=[\n",
        "   {\n",
        "     'role': 'system',\n",
        "     'content': 'You are a helpful assistant with sound mathematical knowledge.',\n",
        "   },\n",
        "   {\n",
        "     'role': 'user',\n",
        "     'content': 'Explain to me the Pythagorean theorem.',\n",
        "   },\n",
        "\n",
        "])\n",
        "# Printing out of the generated response\n",
        "print(ollama_response['message']['content'])"
      ],
      "metadata": {
        "id": "GZege8UfCMC5",
        "outputId": "8d0364e4-9192-49c0-96a6-e4e52a566021",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GZege8UfCMC5",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Pythagoreebian Theorem is a fundamental principle in geometry that describes the relationship between the sides of a right-angled triangle (a triangle where one angle measures 90 degrees). It states that, for any such triangle with perpendicular sides labeled 'a' and 'b', and hypotenuse (the longest side opposite to the right angle) labeled as 'c,'\n",
            "\n",
            "The square of the length of the hypotenuse is equal to the sum of the squares of the other two sides. Mathematically, this can be expressed using the following equation:\n",
            "\n",
            "a² + b² = c²\n",
            "\n",
            "To use the Pythagorean theorem in practice, simply follow these steps:\n",
            "\n",
            "1. Identify a right-angled triangle and label its perpendicular sides as 'a' and 'b,' and the hypotenuse as 'c.'\n",
            "2. Square the lengths of both sides 'a' and 'b', then add their squares together to obtain the value (a² + b²).\n",
            "3. Find the square root of this sum, which gives you the length of side 'c'.\n",
            "\n",
            "Conversely, if you know the lengths of two sides, you can use the Pythagorean theorem to find the third:\n",
            "\n",
            "1. Square the lengths of both known sides (if finding a hypotenuse) or one side and subtract it from the square of the other (if finding a perpenderous side).\n",
            "2. Take the square root of the resulting value, which will give you the length of the missing side.\n",
            "\n",
            "The Pythagorean theorem is widely applicable in various fields such as architecture, engineering, navigation, and more!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68946881",
      "metadata": {
        "id": "68946881"
      },
      "source": [
        "## Create a tool\n",
        "\n",
        "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](/docs/how_to/custom_tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
      "metadata": {
        "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
        "outputId": "c283ff12-c906-46d9-d540-d7764d4f4b05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--\n",
            "multiply\n",
            "Multiply two numbers together.\n",
            "{'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n",
            "--\n",
            "add\n",
            "Add two numbers.\n",
            "{'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(x: float, y: float) -> float:\n",
        "    \"\"\"Multiply two numbers together.\"\"\"\n",
        "    return x * y\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(x: int, y: int) -> int:\n",
        "    \"Add two numbers.\"\n",
        "    return x + y\n",
        "\n",
        "\n",
        "tools = [multiply, add]\n",
        "\n",
        "# Let's inspect the tools\n",
        "for t in tools:\n",
        "    print(\"--\")\n",
        "    print(t.name)\n",
        "    print(t.description)\n",
        "    print(t.args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "be77e780",
      "metadata": {
        "id": "be77e780",
        "outputId": "13dac23e-e3ef-4767-a085-9f308e167322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "multiply.invoke({\"x\": 4, \"y\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
      "metadata": {
        "id": "15dd690e-e54d-4209-91a4-181f69a452ac"
      },
      "source": [
        "## Creating our prompt\n",
        "\n",
        "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2063b564-25ca-4729-a45f-ba4633175b04",
      "metadata": {
        "id": "2063b564-25ca-4729-a45f-ba4633175b04",
        "outputId": "e5511090-bb25-4892-b6ff-04035f4b4a77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiply(x: float, y: float) -> float - Multiply two numbers together.\n",
            "add(x: int, y: int) -> int - Add two numbers.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.tools import render_text_description\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "print(rendered_tools)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import Runnable\n",
        "import ollama\n",
        "\n",
        "class OllamaLLM(Runnable):\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def invoke(self, input, config=None):\n",
        "        # Handle ChatPromptValue object\n",
        "        if hasattr(input, \"to_string\"):\n",
        "            user_input = input.to_string()\n",
        "        else:\n",
        "            user_input = input.get(\"input\", \"\")\n",
        "\n",
        "        response = ollama.chat(model=self.model_name, messages=[\n",
        "            {\n",
        "                'role': 'system',\n",
        "                'content': f\"\"\"\\\n",
        "You are an assistant that has access to the following set of tools.\n",
        "Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use.\n",
        "Return your response as a JSON blob with 'name' and 'arguments' keys.\n",
        "\n",
        "The `arguments` should be a dictionary, with the first key corresponding to 'x' and the second key corresponding to 'y'.\n",
        "\"\"\"\n",
        "            },\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_input,\n",
        "            },\n",
        "        ])\n",
        "        return response['message']['content']\n"
      ],
      "metadata": {
        "id": "LN7eU470DW5i"
      },
      "id": "LN7eU470DW5i",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "f8623e03-60eb-4439-b57b-ecbcebc61b58",
      "metadata": {
        "id": "f8623e03-60eb-4439-b57b-ecbcebc61b58",
        "outputId": "2bbb8993-d3ac-4129-c701-9f82dc45f40b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " {\n",
            "  \"name\": \"add\",\n",
            "  \"arguments\": {\n",
            "    \"x\": 3,\n",
            "    \"y\": 1132\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "User input: {input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Instantiate the OllamaLLM with the desired model name\n",
        "model = OllamaLLM(model_name='phi3:medium')\n",
        "\n",
        "chain = prompt | model\n",
        "message = chain.invoke({\"input\": \"what's 3 plus 1132\"})\n",
        "\n",
        "# Let's take a look at the output from the model\n",
        "# if the model is an LLM (not a chat model), the output will be a string.\n",
        "if isinstance(message, str):\n",
        "    print(message)\n",
        "else:  # Otherwise it's a chat model\n",
        "    print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
      "metadata": {
        "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5"
      },
      "source": [
        "## Adding an output parser\n",
        "\n",
        "We'll use the `JsonOutputParser` for parsing our models output to JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
      "metadata": {
        "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
        "outputId": "7b296582-0cc8-4b2c-e385-6e782b986542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply', 'arguments': {'x': 13, 'y': 4}}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "chain = prompt | model | JsonOutputParser()\n",
        "chain.invoke({\"input\": \"what's thirteen times 4\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
      "metadata": {
        "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83"
      },
      "source": [
        ":::{.callout-important}\n",
        "\n",
        "🎉 Amazing! 🎉 We now instructed our model on how to **request** that a tool be invoked.\n",
        "\n",
        "Now, let's create some logic to actually run the tool!\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
      "metadata": {
        "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc"
      },
      "source": [
        "## Invoking the tool 🏃\n",
        "\n",
        "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke\n",
        "the tool.\n",
        "\n",
        "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "faee95e0-4095-4310-991f-9e9465c6738e",
      "metadata": {
        "id": "faee95e0-4095-4310-991f-9e9465c6738e"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict, Optional, TypedDict\n",
        "import logging\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "class ToolCallRequest(TypedDict):\n",
        "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
        "\n",
        "    name: str\n",
        "    arguments: Dict[str, Any]\n",
        "\n",
        "\n",
        "def invoke_tool(\n",
        "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
        "):\n",
        "    \"\"\"A function that we can use the perform a tool invocation.\n",
        "\n",
        "    Args:\n",
        "        tool_call_request: a dict that contains the keys name and arguments.\n",
        "            The name must match the name of a tool that exists.\n",
        "            The arguments are the arguments to that tool.\n",
        "        config: This is configuration information that LangChain uses that contains\n",
        "            things like callbacks, metadata, etc.See LCEL documentation about RunnableConfig.\n",
        "\n",
        "    Returns:\n",
        "        output from the requested tool\n",
        "    \"\"\"\n",
        "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
        "    name = tool_call_request[\"name\"]\n",
        "    requested_tool = tool_name_to_tool[name]\n",
        "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
      "metadata": {
        "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e"
      },
      "source": [
        "Let's test this out 🧪!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
      "metadata": {
        "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
        "outputId": "6dab3e29-d131-444c-e7be-04eb6c5b06aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.0"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "invoke_tool({\"name\": \"multiply\", \"arguments\": {\"x\": 3, \"y\": 5}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
      "metadata": {
        "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b"
      },
      "source": [
        "## Let's put it together\n",
        "\n",
        "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "0555b384-fde6-4404-86e0-7ea199003d58",
      "metadata": {
        "id": "0555b384-fde6-4404-86e0-7ea199003d58",
        "outputId": "98e81123-8aa9-493a-e195-479c67244966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53.83784653"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
        "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
      "metadata": {
        "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61"
      },
      "source": [
        "## Returning tool inputs\n",
        "\n",
        "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "45404406-859d-4caa-8b9d-5838162c80a0",
      "metadata": {
        "id": "45404406-859d-4caa-8b9d-5838162c80a0",
        "outputId": "7f4591bd-79ac-4446-9450-1f21992a0c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'multiply',\n",
              " 'arguments': {'x': 13, 'y': 4.14137281},\n",
              " 'output': 53.83784653}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    prompt | model | JsonOutputParser() | RunnablePassthrough.assign(output=invoke_tool)\n",
        ")\n",
        "chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
      "metadata": {
        "id": "1797fe82-ea35-4cba-834a-1caf9740d184"
      },
      "source": [
        "## What's next?\n",
        "\n",
        "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
        "\n",
        "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
        "\n",
        "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
        "\n",
        "1. Provide few shot examples.\n",
        "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}